{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dea9f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for AI module: fdd_utils/ai_helper.py\n",
    "from fdd_utils.ai_helper import AIHelper\n",
    " \n",
    "# Define system prompt and user prompt\n",
    "system_prompt = r\"\"\"You are a helpful assistant.\"\"\"\n",
    "user_prompt = r\"\"\"Whehre is HK?\"\"\"\n",
    " \n",
    "# Choose the model type ('openai', 'local', 'deepseek')\n",
    "model_type = \"local\"\n",
    " \n",
    "# Create an instance of the AIHelper\n",
    "ai_helper = AIHelper(model_type=model_type, user_prompt=user_prompt, system_prompt=system_prompt)\n",
    " \n",
    "# Get the response from the AI model\n",
    "response = ai_helper.get_response()\n",
    "print(response)\n",
    " \n",
    "# AI module test pass\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13241ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databook handling\n",
    "from fdd_utils.process_databook import extract_data_from_excel\n",
    "import warnings\n",
    " \n",
    "# Ignore the user warning for data validation\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    " \n",
    "# Usage:\n",
    "# Eng (Haining 3 entites): 221128.Project TK.Databook.JW.xlsx\n",
    "# Semi Databook: deli.xlsx\n",
    "databook_path = \"inputs/221128.Project TK.Databook.JW.xlsx\"\n",
    "# Chi (single entity): 240624.ËÅîÊ¥ã-databook.xlsx || 240627.‰∏úËéûÂ≤≠Âçó-databook.xlsx\n",
    "# databook_path = \"inputs/240627.‰∏úËéûÂ≤≠Âçó-databook.xlsx\"\n",
    "entity_name = 'Haining Wanpu' # Lianyang, Dongguan Lingnan\n",
    "mode = \"BS\" # BS, IS, All\n",
    " \n",
    "dfs, workbook_list, result_type, report_language = extract_data_from_excel(databook_path, entity_name, mode)\n",
    " \n",
    "# workbook_list\n",
    "print('Key list:', workbook_list)\n",
    " \n",
    "# Example databook data for single item\n",
    "# dfs['Cash']\n",
    " \n",
    "# Excel processing module ok, individual steps inside workings/process_databook.ipynb\n",
    "# Results -> Pass\n",
    "# Remark: here key list = workbook tabname\n",
    "# ËÅîÊ¥ã, ‰∏úËéûÂ≤≠Âçó IS, BS - pass\n",
    " \n",
    "# one issue: ÂèØËÉΩÊúÉÊúâsubtotal+Á¥∞È†ÖÁöÑË°®ÈÅîÊñπÊ≥ï, ‰πãÂæåË¶ÅÂ∞èÂøÉÔºåÂ¶ÇÊûúAIÊúâbugË¶ÅËøîÂë¢Â∫¶Êîπ\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf9c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['Cash']\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622b0d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_language\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f4242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key reconciliation\n",
    " \n",
    "# mapping.json\n",
    "# \"Cash\", \"AR\", \"Prepayments\", \"OR\", \"Inventory\", \"IP\", \"IA\", \"NCA\", \"NCL\", \"Other NCA\", \"Other CA\", \"AP\", \"Advances\",\n",
    "# \"Tax payable\", \"OP\", \"Capital\", \"Reserve\", \"Capital reserve\", \"R/E\", \"OI\", \"OC\", \"Tax and Surcharges\", \"GA\",\n",
    "# \"Fin Exp\",  \"Cr Loss\", \"Other Income\", \"Non-operating Income\", \"Non-operating Exp\", \"Income tax\", \"LT DTA\", \"Selling cost\"\n",
    " \n",
    "# pattern.json\n",
    "# \"Cash\", \"AR\", \"Prepayments\", \"OR\", \"IP\", \"NCA\", \"Other NCA\", \"Other CA\", \"AP\", ## Â∞ëÂ∑¶: \"Inventory\"Ôºå \"IA\"Ôºå \"NCL\", \"Advances\"\n",
    "# \"Tax payable\", \"OP\", \"Capital\", \"Reserve\", \"OI\", \"OC\", \"Tax and Surcharges\", \"GA\", ## Â∞ëÂ∑¶: \"Capital reserve\", \"R/E\"\n",
    "# \"Fin Exp\", \"Cr Loss\", \"Other Income\", \"Non-operating Income\", \"Non-operating Exp\", \"Income tax\", \"LT DTA\" ## Â∞ëÂ∑¶: \"Capital reserve\", \"R/E\", \"Selling cost\"\n",
    " \n",
    "# prompts.json\n",
    "# \"Cash\", \"AR\", \"Other NCA\", \"Other CA\", \"AP\" ## Â∞ëÂ∑¶: \"Prepayments\", \"OR\", \"Inventory\", \"IP\", \"IA\", \"NCA\", \"NCL\", \"Advances\"\n",
    "# \"Reserve\", \"Capital reserve\", \"R/E\", \"OI\", \"OC\", \"Tax and Surcharges\", \"GA\", ## Â∞ëÂ∑¶: \"Tax payable\", \"OP\", \"Capital\",\n",
    "# \"Fin Exp\",  \"Cr Loss\", \"Other Income\", \"Non-operating Income\", \"Non-operating Exp\", \"Income tax\", \"LT DTA\", ## Â∞ëÂ∑¶: \"Selling cost\"\n",
    " \n",
    "# Âà∞ÊôÇË¶ÅÂä†Ëøî‰∏äÂéª ‰ª•mappingsÁà≤Ê∫ñ\n",
    "# Keys checked identical    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693fd148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Agent Áî±Âë¢Â∫¶ÈñãÂßãË¶Å+language\n",
    " \n",
    "# Agent 1: Ê†πÊìöacctÊÉÖÊ≥ÅÊèÄpattern+draft Input=language, key(account, pattern) Output=key+AI output\n",
    "# Agent 2ÔºöÂ∞çÊï∏Â≠ó(+desc)‰øÇÂí™Â≤© Input=AI output + key(accounts)\n",
    "# Agent 3: compliance -> Ê™¢Êü•ÂÜÖÂÆπ‰øÇÂí™make sense\n",
    "# Agent 4: translate\n",
    " \n",
    "# Áõ°Èáè+heuristic features Â∞§ÂÖ∂on ÂÆπÊòìÁùáÂà∞ÂïèÈ°åÂòÖ‰ΩçÁΩÆ\n",
    " \n",
    "# Expand options: 1. industry 2.RAG\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fd1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fdd_utils.content_generation import content_generate\n",
    " \n",
    "agent = 'agent_1'\n",
    "model_name = 'local'\n",
    "report_language = 'Eng' # Eng, Chi\n",
    " \n",
    "content_generate(agent, workbook_list, model_name, report_language, dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0259df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Run Agent 2 to verify and refine the output of Agent 1\n",
    "for mapping_key in mapping_keys:\n",
    "    result = load_and_verify(agent2, mapping_key)  # Implement a function to load and check results\n",
    " \n",
    "    if result:\n",
    "        # Implement AI processing for verification and refinement\n",
    "        system_prompt = f\"System prompt for {agent2} verification.\"\n",
    "        user_prompt = f\"User prompt containing data verification specifics for {mapping_key}.\"\n",
    "           \n",
    "        ai_helper = AIHelper(model_type=model_name, user_prompt=user_prompt, system_prompt=system_prompt)\n",
    "        verification_response = ai_helper.get_response().strip()\n",
    " \n",
    "        # Final verification step: ensure format compliance and log or output appropriately\n",
    "        verification_response = verification_response.replace(\"\\n\\n\", \"\\n\").replace(\"\\n \\n\", \"\\n\")\n",
    "        print(f\"Verified Output for key {mapping_key}: {verification_response}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfac1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml\n",
    " \n",
    "def read_result_by_key(mapping_key, log_dir='fdd_utils'):\n",
    "    intermediate_file = os.path.join(log_dir, 'intermediate_results.yml')\n",
    "    if not os.path.exists(intermediate_file):\n",
    "        print(\"Intermediate file does not exist.\")\n",
    "        return None\n",
    " \n",
    "    with open(intermediate_file, 'r', encoding='utf-8') as file:\n",
    "        data = yaml.safe_load(file) or {}\n",
    " \n",
    "    result_data = data.get(mapping_key, None)\n",
    "    return result_data.get('result') if result_data else None\n",
    " \n",
    "# Example usage to read the result by key\n",
    "result = read_result_by_key('AR')\n",
    "print(result)\n",
    " \n",
    "# ['Cash', 'AR', 'Prepayments', 'OR', 'Other current assets', 'Investment properties', 'Tax payable', 'OP', 'AP', 'Share capital']\n",
    "# AR - Haining issue\n",
    "# General: 1 d.p., 1000Kshould = M 3. all amount should be CNYxxxK or million\n",
    " \n",
    "# Agent 1 completed with some issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d28dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Test may start here #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0970e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Test New AIHelper with Agent Support\n",
    "# Test for enhanced AI module: fdd_utils/ai_helper.py\n",
    "from fdd_utils.ai_helper import AIHelper\n",
    "\n",
    "# Test 1: Basic AIHelper with agent\n",
    "print(\"=\"*60)\n",
    "print(\"TEST 1: Basic AIHelper with Agent Support\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create an instance with agent_1\n",
    "ai_helper = AIHelper(\n",
    "    model_type='local',  # or 'deepseek', 'openai'\n",
    "    agent_name='agent_1',\n",
    "    language='Eng',\n",
    "    use_heuristic=False\n",
    ")\n",
    "\n",
    "# Simple test prompt\n",
    "user_prompt = \"Where is Hong Kong?\"\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "# Get response\n",
    "response = ai_helper.get_response(user_prompt, system_prompt)\n",
    "\n",
    "print(\"\\nResponse content:\", response['content'])\n",
    "print(f\"Mode: {response['mode']}\")\n",
    "print(f\"Duration: {response['duration']:.2f}s\")\n",
    "print(f\"Tokens: {response['tokens_used']}\")\n",
    "print(\"\\n‚úÖ AIHelper test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822455b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Test Databook Extraction (Should Still Work)\n",
    "# Test databook handling with language detection\n",
    "from fdd_utils.process_databook import extract_data_from_excel\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST 2: Databook Extraction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Your existing paths\n",
    "databook_path = \"inputs/221128.Project TK.Databook.JW.xlsx\"\n",
    "entity_name = 'Haining Wanpu'\n",
    "mode = \"BS\"  # BS, IS, All\n",
    "\n",
    "# Extract data (same as before, but now with better language detection)\n",
    "dfs, workbook_list, result_type, report_language = extract_data_from_excel(\n",
    "    databook_path, entity_name, mode\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Extracted {len(workbook_list)} worksheets\")\n",
    "print(f\"Result type: {result_type}\")\n",
    "print(f\"Detected language: {report_language}\")\n",
    "print(f\"Keys: {workbook_list[:5]}...\")  # Show first 5\n",
    "\n",
    "# Check a sample\n",
    "if 'Cash' in dfs:\n",
    "    print(f\"\\nSample data (Cash):\")\n",
    "    print(dfs['Cash'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b211afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Test New AI Pipeline - Quick Method\n",
    "# Test the new AI pipeline - Quick method\n",
    "from fdd_utils.ai_pipeline import run_quick_pipeline\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST 3: Quick Pipeline Execution\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Quick test with heuristic mode (no API calls, fast)\n",
    "print(\"\\nRunning with HEURISTIC mode (no AI calls)...\")\n",
    "results = run_quick_pipeline(\n",
    "    databook_path=databook_path,\n",
    "    entity_name=entity_name,\n",
    "    mode='BS',\n",
    "    model_type='local',\n",
    "    language=report_language or 'Eng'\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Quick pipeline completed!\")\n",
    "print(f\"Processed {len(results)} items\")\n",
    "\n",
    "# Show first result\n",
    "first_key = list(results.keys())[0] if results else None\n",
    "if first_key:\n",
    "    print(f\"\\nSample result for '{first_key}':\")\n",
    "    print(results[first_key][:200] + \"...\" if len(results[first_key]) > 200 else results[first_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd00fe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Test Full Pipeline with Orchestrator\n",
    "# Test with the orchestrator (recommended method)\n",
    "from fdd_utils.ai_pipeline import AIPipelineOrchestrator\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST 4: Full AI Pipeline with Orchestrator\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize orchestrator\n",
    "orchestrator = AIPipelineOrchestrator(config_path='fdd_utils/config.yml')\n",
    "\n",
    "# Load data (reusing the variables from Cell 2)\n",
    "print(\"\\nLoading data...\")\n",
    "data_info = orchestrator.load_data(\n",
    "    databook_path=databook_path,\n",
    "    entity_name=entity_name,\n",
    "    mode='BS'\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(data_info['mapping_keys'])} worksheets\")\n",
    "print(f\"Detected language: {data_info['report_language']}\")\n",
    "\n",
    "# Run full pipeline with HEURISTIC mode first (for testing, no API costs)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Running FULL PIPELINE (Heuristic Mode - Testing)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = orchestrator.run_full_pipeline(\n",
    "    model_type='local',\n",
    "    language=data_info['report_language'],\n",
    "    use_heuristic=True,  # Set to False to use real AI\n",
    "    use_multithreading=True,\n",
    "    max_workers=4\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "orchestrator.print_results_summary()\n",
    "\n",
    "# Save results\n",
    "orchestrator.save_results('fdd_utils/output/test_results.yml')\n",
    "print(\"\\n‚úÖ Full pipeline test completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925411d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Test Individual Agents\n",
    "# Test running individual agents\n",
    "print(\"=\"*60)\n",
    "print(\"TEST 5: Individual Agents\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reinitialize orchestrator with fresh data\n",
    "orchestrator = AIPipelineOrchestrator()\n",
    "orchestrator.load_data(databook_path, entity_name, 'BS')\n",
    "\n",
    "# Run only Agent 1 (Content Generator)\n",
    "print(\"\\n--- Running Agent 1 Only ---\")\n",
    "results_agent_1 = orchestrator.run_agent_only(\n",
    "    agent_name='agent_1',\n",
    "    model_type='local',\n",
    "    language='Eng',\n",
    "    use_heuristic=True  # Set to False for real AI\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Agent 1 completed: {len(results_agent_1)} items\")\n",
    "\n",
    "# Show a sample result\n",
    "sample_key = list(results_agent_1.keys())[0]\n",
    "print(f\"\\nAgent 1 output for '{sample_key}':\")\n",
    "print(results_agent_1[sample_key][:300] + \"...\")\n",
    "\n",
    "# Run Agent 2 with Agent 1's results\n",
    "print(\"\\n--- Running Agent 2 Only (with Agent 1 results) ---\")\n",
    "results_agent_2 = orchestrator.run_agent_only(\n",
    "    agent_name='agent_2',\n",
    "    previous_results=results_agent_1,\n",
    "    model_type='local',\n",
    "    language='Eng',\n",
    "    use_heuristic=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Agent 2 completed: {len(results_agent_2)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32378947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Test Sequential Agents\n",
    "# Test running specific agents in sequence\n",
    "print(\"=\"*60)\n",
    "print(\"TEST 6: Sequential Agents (Custom Order)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "orchestrator = AIPipelineOrchestrator()\n",
    "orchestrator.load_data(databook_path, entity_name, 'BS')\n",
    "\n",
    "# Run only agents 1, 2, and 4 (skip agent 3 if desired)\n",
    "results = orchestrator.run_sequential_agents(\n",
    "    agents=['agent_1', 'agent_2', 'agent_4'],  # Custom agent sequence\n",
    "    model_type='local',\n",
    "    language='Eng',\n",
    "    use_heuristic=True  # Set to False for real AI\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Sequential agents completed: {len(results)} items\")\n",
    "\n",
    "# Compare with full pipeline\n",
    "orchestrator.print_results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3247f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Test with REAL AI (DeepSeek)\n",
    "# Test with real AI (replace heuristic with actual AI)\n",
    "print(\"=\"*60)\n",
    "print(\"TEST 7: Real AI Pipeline (DeepSeek)\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚ö†Ô∏è  This will use API credits!\")\n",
    "\n",
    "# Make sure your API key is configured in config.yml first!\n",
    "confirm = input(\"Run with real AI? This will use API credits. (yes/no): \")\n",
    "\n",
    "if confirm.lower() == 'yes':\n",
    "    orchestrator = AIPipelineOrchestrator()\n",
    "    orchestrator.load_data(databook_path, entity_name, 'BS')\n",
    "    \n",
    "    # Run with REAL AI\n",
    "    results = orchestrator.run_full_pipeline(\n",
    "        model_type='local',  # or 'deepseek' if configured\n",
    "        language=report_language or 'Eng',\n",
    "        use_heuristic=False,  # Real AI!\n",
    "        use_multithreading=True,\n",
    "        max_workers=4\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    orchestrator.save_results('fdd_utils/output/real_ai_results.yml')\n",
    "    orchestrator.print_results_summary()\n",
    "    \n",
    "    # Show a detailed result\n",
    "    sample_key = 'Cash' if 'Cash' in results else list(results.keys())[0]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Detailed result for '{sample_key}':\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(results[sample_key])\n",
    "    \n",
    "    print(\"\\n‚úÖ Real AI pipeline completed!\")\n",
    "else:\n",
    "    print(\"Skipped real AI test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795462dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: View Results and Logs\n",
    "# View results and check logs\n",
    "import yaml\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST 8: View Results and Logs\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check intermediate results (if exists from old system)\n",
    "intermediate_file = 'fdd_utils/intermediate_results.yml'\n",
    "if os.path.exists(intermediate_file):\n",
    "    with open(intermediate_file, 'r', encoding='utf-8') as f:\n",
    "        old_results = yaml.safe_load(f)\n",
    "    print(f\"\\nüìÅ Old intermediate results: {len(old_results)} items\")\n",
    "\n",
    "# Check new log files\n",
    "log_dir = 'fdd_utils/logs'\n",
    "if os.path.exists(log_dir):\n",
    "    log_files = sorted(glob(os.path.join(log_dir, 'ai_processing_*.log')))\n",
    "    data_files = sorted(glob(os.path.join(log_dir, 'ai_data_*.yml')))\n",
    "    \n",
    "    print(f\"\\nüìã Log files: {len(log_files)}\")\n",
    "    print(f\"üìä Data files: {len(data_files)}\")\n",
    "    \n",
    "    # Show latest log file name\n",
    "    if log_files:\n",
    "        latest_log = log_files[-1]\n",
    "        print(f\"\\nLatest log: {os.path.basename(latest_log)}\")\n",
    "        \n",
    "        # Read last 20 lines\n",
    "        with open(latest_log, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            print(\"\\nLast 20 lines of log:\")\n",
    "            print(\"\".join(lines[-20:]))\n",
    "    \n",
    "    # Show latest data file summary\n",
    "    if data_files:\n",
    "        latest_data = data_files[-1]\n",
    "        print(f\"\\nüìä Latest data file: {os.path.basename(latest_data)}\")\n",
    "        \n",
    "        with open(latest_data, 'r', encoding='utf-8') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "            \n",
    "        if 'summary' in data:\n",
    "            print(\"\\nSummary:\")\n",
    "            for key, value in data['summary'].items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "# Check saved results\n",
    "output_file = 'fdd_utils/output/test_results.yml'\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        results = yaml.safe_load(f)\n",
    "    print(f\"\\n‚úÖ Saved results file exists: {len(results)} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818147e",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# COMPREHENSIVE TESTING SECTION\n",
    "# ========================================\n",
    "# The cells below provide comprehensive testing of the improved AI pipeline\n",
    "# with fixed agent prompts (agents 2, 3, 4 now output only content, no meta-commentary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcee138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Test Data Loading and Language Detection\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 1: Data Loading and Language Detection\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from fdd_utils.process_databook import extract_data_from_excel\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "# Test with English databook\n",
    "databook_path = \"inputs/221128.Project TK.Databook.JW.xlsx\"\n",
    "entity_name = 'Haining Wanpu'\n",
    "mode = \"BS\"  # BS, IS, All\n",
    "\n",
    "# Extract data\n",
    "dfs, workbook_list, result_type, report_language = extract_data_from_excel(\n",
    "    databook_path, entity_name, mode\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data Loading Results:\")\n",
    "print(f\"   - Extracted worksheets: {len(workbook_list)}\")\n",
    "print(f\"   - Result type: {result_type}\")\n",
    "print(f\"   - Detected language: {report_language}\")\n",
    "print(f\"   - Worksheet keys: {workbook_list}\")\n",
    "\n",
    "# Display sample data\n",
    "if 'Cash' in dfs:\n",
    "    print(f\"\\nüìä Sample data for 'Cash':\")\n",
    "    print(dfs['Cash'])\n",
    "    \n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d3e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Test Full AI Pipeline with HEURISTIC Mode (Fast Testing)\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 2: Full AI Pipeline - HEURISTIC Mode (No API Calls)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from fdd_utils.content_generation import run_ai_pipeline, save_results\n",
    "import os\n",
    "\n",
    "# Run pipeline in HEURISTIC mode (no actual AI calls, instant testing)\n",
    "print(\"\\nüîÑ Running 4-agent pipeline in HEURISTIC mode...\")\n",
    "print(\"   This tests the pipeline structure without making API calls\\n\")\n",
    "\n",
    "results_heuristic = run_ai_pipeline(\n",
    "    mapping_keys=workbook_list[:3],  # Test with first 3 items\n",
    "    dfs=dfs,\n",
    "    model_type='local',\n",
    "    language=report_language,\n",
    "    use_heuristic=True,  # HEURISTIC mode = no API calls\n",
    "    use_multithreading=True,\n",
    "    max_workers=4\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ HEURISTIC Pipeline Results:\")\n",
    "print(f\"   - Items processed: {len(results_heuristic)}\")\n",
    "print(f\"   - Agents per item: {list(results_heuristic[workbook_list[0]].keys()) if results_heuristic else 'None'}\")\n",
    "\n",
    "# Check structure\n",
    "if results_heuristic and workbook_list[0] in results_heuristic:\n",
    "    sample_key = workbook_list[0]\n",
    "    print(f\"\\nüìã Sample result structure for '{sample_key}':\")\n",
    "    for agent, content in results_heuristic[sample_key].items():\n",
    "        print(f\"   - {agent}: {len(content)} chars\")\n",
    "        if agent == 'final':\n",
    "            print(f\"      Preview: {content[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcae0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Test with REAL AI (Local Model - Single Item)\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 3: Real AI Pipeline - Single Item Test\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚ö†Ô∏è  This will make actual AI API calls to local model\")\n",
    "print(\"\")\n",
    "\n",
    "# Confirm before running\n",
    "import sys\n",
    "confirm = input(\"Run with REAL AI? This will use local AI model. (yes/no): \")\n",
    "\n",
    "if confirm.lower() == 'yes':\n",
    "    print(\"\\nüîÑ Running pipeline with REAL AI (Local Model)...\\n\")\n",
    "    \n",
    "    # Test with just ONE item first\n",
    "    test_keys = [workbook_list[0]]  # Test with 'Cash' only\n",
    "    test_dfs = {k: dfs[k] for k in test_keys}\n",
    "    \n",
    "    results_real_ai = run_ai_pipeline(\n",
    "        mapping_keys=test_keys,\n",
    "        dfs=test_dfs,\n",
    "        model_type='local',  # Using local model\n",
    "        language=report_language,\n",
    "        use_heuristic=False,  # REAL AI!\n",
    "        use_multithreading=False,  # Single-threaded for testing\n",
    "        max_workers=1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Real AI Pipeline Results:\")\n",
    "    print(f\"   - Items processed: {len(results_real_ai)}\")\n",
    "    \n",
    "    # Display detailed results\n",
    "    if results_real_ai and test_keys[0] in results_real_ai:\n",
    "        sample_key = test_keys[0]\n",
    "        print(f\"\\nüìÑ Detailed results for '{sample_key}':\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for agent_name in ['agent_1', 'agent_2', 'agent_3', 'agent_4', 'final']:\n",
    "            if agent_name in results_real_ai[sample_key]:\n",
    "                content = results_real_ai[sample_key][agent_name]\n",
    "                print(f\"\\n{agent_name.upper()}:\")\n",
    "                print(f\"Length: {len(content)} chars\")\n",
    "                print(f\"Content: {content}\")\n",
    "                print(\"-\"*70)\n",
    "        \n",
    "        # Check for meta-commentary (should be removed by clean_agent_output)\n",
    "        final_content = results_real_ai[sample_key].get('final', '')\n",
    "        meta_keywords = ['verified', 'corrected', 'refined', 'formatted', 'output:', \n",
    "                        'Â∑≤È™åËØÅ', 'Â∑≤Êõ¥Ê≠£', 'ÁªèËøá']\n",
    "        found_meta = [kw for kw in meta_keywords if kw.lower() in final_content.lower()]\n",
    "        \n",
    "        if found_meta:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: Found potential meta-commentary: {found_meta}\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ No meta-commentary detected in final output!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è  Skipped real AI test.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c285378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Test Full Pipeline with ALL Items (Real AI)\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 4: Full Pipeline with ALL Items - Real AI\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚ö†Ô∏è  This will process ALL items with real AI - will take time!\")\n",
    "print(f\"    Total items to process: {len(workbook_list)}\")\n",
    "print(\"\")\n",
    "\n",
    "confirm = input(\"Run FULL pipeline with REAL AI? (yes/no): \")\n",
    "\n",
    "if confirm.lower() == 'yes':\n",
    "    print(\"\\nüîÑ Running FULL pipeline with REAL AI...\")\n",
    "    print(\"   Using multi-threading for faster processing\\n\")\n",
    "    \n",
    "    results_full = run_ai_pipeline(\n",
    "        mapping_keys=workbook_list,\n",
    "        dfs=dfs,\n",
    "        model_type='local',\n",
    "        language=report_language,\n",
    "        use_heuristic=False,  # REAL AI\n",
    "        use_multithreading=True,  # Multi-threading enabled\n",
    "        max_workers=4\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Full Pipeline Completed!\")\n",
    "    print(f\"   - Total items processed: {len(results_full)}\")\n",
    "    print(f\"   - Successful: {sum(1 for v in results_full.values() if 'final' in v)}\")\n",
    "    print(f\"   - Failed: {sum(1 for v in results_full.values() if 'final' not in v)}\")\n",
    "    \n",
    "    # Save results\n",
    "    output_path = 'fdd_utils/output/results.yml'\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    save_results(results_full, output_path)\n",
    "    \n",
    "    print(f\"\\nüíæ Results saved to: {output_path}\")\n",
    "    \n",
    "    # Display summary of first 3 items\n",
    "    print(f\"\\nüìã Summary of First 3 Items:\")\n",
    "    print(\"=\"*70)\n",
    "    for i, key in enumerate(list(results_full.keys())[:3], 1):\n",
    "        if 'final' in results_full[key]:\n",
    "            final = results_full[key]['final']\n",
    "            print(f\"\\n{i}. {key}:\")\n",
    "            print(f\"   Length: {len(final)} chars\")\n",
    "            print(f\"   Preview: {final[:150]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "else:\n",
    "    print(\"\\n‚è≠Ô∏è  Skipped full pipeline test.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3414ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Analyze Results and Check for Issues\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 5: Analyze Results and Validate Output Quality\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# Check if results.yml exists\n",
    "results_path = 'fdd_utils/output/results.yml'\n",
    "\n",
    "if os.path.exists(results_path):\n",
    "    print(f\"\\nüìä Loading results from: {results_path}\\n\")\n",
    "    \n",
    "    with open(results_path, 'r', encoding='utf-8') as f:\n",
    "        results = yaml.safe_load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(results)} items from results.yml\\n\")\n",
    "    \n",
    "    # Analyze each item\n",
    "    print(\"=\"*70)\n",
    "    print(\"QUALITY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    issues_found = []\n",
    "    \n",
    "    for key, agents_output in results.items():\n",
    "        print(f\"\\nüìå {key}:\")\n",
    "        \n",
    "        # Check if all agents completed\n",
    "        expected_agents = ['agent_1', 'agent_2', 'agent_3', 'agent_4', 'final']\n",
    "        missing_agents = [a for a in expected_agents if a not in agents_output]\n",
    "        \n",
    "        if missing_agents:\n",
    "            print(f\"   ‚ö†Ô∏è  Missing agents: {missing_agents}\")\n",
    "            issues_found.append(f\"{key}: Missing agents {missing_agents}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ All agents completed\")\n",
    "        \n",
    "        # Check agent_2 and agent_3 for meta-commentary\n",
    "        for agent_name in ['agent_2', 'agent_3', 'agent_4', 'final']:\n",
    "            if agent_name in agents_output:\n",
    "                content = agents_output[agent_name]\n",
    "                \n",
    "                # Check for meta-commentary keywords\n",
    "                meta_patterns = [\n",
    "                    'verified output:', 'corrected output:', 'refined output:',\n",
    "                    'after verification', 'after refining', 'corrections made:',\n",
    "                    'i verified', 'i corrected', 'i refined',\n",
    "                    'Â∑≤È™åËØÅËæìÂá∫', 'Â∑≤Êõ¥Ê≠£ËæìÂá∫', 'ÁªèËøáÈ™åËØÅ', 'ÁªèËøáÁ≤æÁÇº', 'ÊâÄÂÅöÊõ¥Ê≠£'\n",
    "                ]\n",
    "                \n",
    "                found_meta = [p for p in meta_patterns if p in content.lower()]\n",
    "                \n",
    "                if found_meta:\n",
    "                    print(f\"   ‚ö†Ô∏è  {agent_name}: Found meta-commentary: {found_meta}\")\n",
    "                    issues_found.append(f\"{key} - {agent_name}: Meta-commentary found\")\n",
    "                else:\n",
    "                    print(f\"   ‚úÖ {agent_name}: Clean output (no meta-commentary)\")\n",
    "        \n",
    "        # Check final output length\n",
    "        if 'final' in agents_output:\n",
    "            final_len = len(agents_output['final'])\n",
    "            print(f\"   üìè Final output length: {final_len} chars\")\n",
    "            \n",
    "            if final_len < 50:\n",
    "                print(f\"   ‚ö†Ô∏è  Warning: Final output seems too short\")\n",
    "                issues_found.append(f\"{key}: Final output too short ({final_len} chars)\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if issues_found:\n",
    "        print(f\"\\n‚ö†Ô∏è  Found {len(issues_found)} issues:\")\n",
    "        for issue in issues_found:\n",
    "            print(f\"   - {issue}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ NO ISSUES FOUND! All outputs are clean and complete.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Results file not found: {results_path}\")\n",
    "    print(\"   Please run Test 4 first to generate results.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cb32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: View Logs and Detailed Agent Outputs\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 6: View Logs and Agent Processing Details\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# Find latest log files\n",
    "log_dir = 'fdd_utils/logs'\n",
    "\n",
    "if os.path.exists(log_dir):\n",
    "    log_files = sorted(glob.glob(os.path.join(log_dir, 'ai_processing_*.log')))\n",
    "    data_files = sorted(glob.glob(os.path.join(log_dir, 'ai_data_*.yml')))\n",
    "    \n",
    "    print(f\"\\nüìã Found {len(log_files)} log files\")\n",
    "    print(f\"üìä Found {len(data_files)} data files\\n\")\n",
    "    \n",
    "    # Show latest log file\n",
    "    if log_files:\n",
    "        latest_log = log_files[-1]\n",
    "        print(f\"üìÑ Latest log file: {os.path.basename(latest_log)}\")\n",
    "        \n",
    "        # Read last 30 lines\n",
    "        with open(latest_log, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            print(\"\\nLast 30 lines:\")\n",
    "            print(\"=\"*70)\n",
    "            print(\"\".join(lines[-30:]))\n",
    "    \n",
    "    # Show latest data file summary\n",
    "    if data_files:\n",
    "        latest_data = data_files[-1]\n",
    "        print(f\"\\nüìä Latest data file: {os.path.basename(latest_data)}\")\n",
    "        \n",
    "        with open(latest_data, 'r', encoding='utf-8') as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        \n",
    "        if 'summary' in data:\n",
    "            print(\"\\nüìà Pipeline Summary:\")\n",
    "            print(\"=\"*70)\n",
    "            for key, value in data['summary'].items():\n",
    "                print(f\"   {key}: {value}\")\n",
    "        \n",
    "        # Show agent outputs for first item\n",
    "        if 'processing_results' in data:\n",
    "            first_key = list(data['processing_results'].keys())[0]\n",
    "            print(f\"\\nüìù Detailed Output for First Item: '{first_key}'\")\n",
    "            print(\"=\"*70)\n",
    "            \n",
    "            for agent_name, agent_data in data['processing_results'][first_key].items():\n",
    "                if 'output' in agent_data:\n",
    "                    print(f\"\\n{agent_name.upper()}:\")\n",
    "                    print(f\"   Duration: {agent_data.get('duration', 0):.2f}s\")\n",
    "                    print(f\"   Tokens: {agent_data.get('tokens_used', 0)}\")\n",
    "                    print(f\"   Mode: {agent_data.get('mode', 'unknown')}\")\n",
    "                    print(f\"   Output length: {agent_data.get('content_length', 0)} chars\")\n",
    "                    print(f\"   Output preview: {agent_data['output'][:150]}...\")\n",
    "                    print(\"-\"*70)\n",
    "else:\n",
    "    print(f\"\\n‚ùå Log directory not found: {log_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4da429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Compare Agent Outputs (Show Evolution)\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 7: Compare Agent Outputs - Content Evolution\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_path = 'fdd_utils/output/results.yml'\n",
    "\n",
    "if os.path.exists(results_path):\n",
    "    with open(results_path, 'r', encoding='utf-8') as f:\n",
    "        results = yaml.safe_load(f)\n",
    "    \n",
    "    # Pick first item with all agents\n",
    "    sample_key = None\n",
    "    for key, agents in results.items():\n",
    "        if all(agent in agents for agent in ['agent_1', 'agent_2', 'agent_3', 'agent_4', 'final']):\n",
    "            sample_key = key\n",
    "            break\n",
    "    \n",
    "    if sample_key:\n",
    "        print(f\"\\nüìä Content Evolution for: '{sample_key}'\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        agents_data = results[sample_key]\n",
    "        \n",
    "        for agent_name in ['agent_1', 'agent_2', 'agent_3', 'agent_4', 'final']:\n",
    "            if agent_name in agents_data:\n",
    "                content = agents_data[agent_name]\n",
    "                print(f\"\\n{agent_name.upper()}:\")\n",
    "                print(f\"   Length: {len(content)} chars\")\n",
    "                print(f\"   Content:\\n   {content}\")\n",
    "                print(\"-\"*70)\n",
    "        \n",
    "        # Show length changes\n",
    "        print(\"\\nüìè Length Evolution:\")\n",
    "        lengths = {agent: len(agents_data[agent]) for agent in ['agent_1', 'agent_2', 'agent_3', 'agent_4', 'final'] if agent in agents_data}\n",
    "        for agent, length in lengths.items():\n",
    "            print(f\"   {agent}: {length} chars\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No item found with all agents completed\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Results file not found: {results_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c026eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Extract Final Contents for Use\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 8: Extract Final Contents for Further Processing\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from fdd_utils.content_generation import extract_final_contents\n",
    "\n",
    "results_path = 'fdd_utils/output/results.yml'\n",
    "\n",
    "if os.path.exists(results_path):\n",
    "    with open(results_path, 'r', encoding='utf-8') as f:\n",
    "        results = yaml.safe_load(f)\n",
    "    \n",
    "    # Extract final contents only\n",
    "    final_contents = extract_final_contents(results)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Extracted {len(final_contents)} final contents\")\n",
    "    print(\"\\nüìÑ Final Contents:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for key, content in final_contents.items():\n",
    "        print(f\"\\n{key}:\")\n",
    "        print(f\"   {content}\")\n",
    "        print(\"-\"*70)\n",
    "    \n",
    "    # Save final contents separately\n",
    "    final_output_path = 'fdd_utils/output/final_contents.yml'\n",
    "    with open(final_output_path, 'w', encoding='utf-8') as f:\n",
    "        yaml.dump(final_contents, f, default_flow_style=False, allow_unicode=True)\n",
    "    \n",
    "    print(f\"\\nüíæ Final contents saved to: {final_output_path}\")\n",
    "    print(\"\\n‚úÖ These final contents can now be used for PowerPoint generation!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ùå Results file not found: {results_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae418fc2",
   "metadata": {},
   "source": [
    "## üéØ TESTING COMPLETE!\n",
    "\n",
    "### Summary of Tests:\n",
    "1. ‚úÖ **Test 1**: Data loading and language detection\n",
    "2. ‚úÖ **Test 2**: Full AI pipeline (HEURISTIC mode - fast testing)\n",
    "3. ‚úÖ **Test 3**: Real AI pipeline (Single item - with local model)\n",
    "4. ‚úÖ **Test 4**: Full pipeline with ALL items (Real AI)\n",
    "5. ‚úÖ **Test 5**: Analyze results and validate output quality\n",
    "6. ‚úÖ **Test 6**: View logs and detailed agent outputs\n",
    "7. ‚úÖ **Test 7**: Compare agent outputs (content evolution)\n",
    "8. ‚úÖ **Test 8**: Extract final contents for further processing\n",
    "\n",
    "### Key Improvements Made:\n",
    "- ‚úÖ Fixed Agent 2 & 3 prompts to output ONLY revised content (no meta-commentary)\n",
    "- ‚úÖ Clarified Agent 4 purpose and output format\n",
    "- ‚úÖ Added `clean_agent_output()` function to remove any remaining meta-commentary\n",
    "- ‚úÖ Improved error handling and logging\n",
    "- ‚úÖ Added comprehensive testing suite\n",
    "\n",
    "### What is agent_4 and 'final' for?\n",
    "- **agent_4**: Format Checker - performs final formatting checks (currency format, quotation marks, numbering format, language consistency)\n",
    "- **final**: Stores the agent_4 output as the final result ready for use in PowerPoint generation\n",
    "\n",
    "### Files Generated:\n",
    "- `fdd_utils/output/results.yml` - Full results with all agent outputs\n",
    "- `fdd_utils/output/final_contents.yml` - Only final contents for PowerPoint\n",
    "- `fdd_utils/logs/ai_processing_*.log` - Processing logs\n",
    "- `fdd_utils/logs/ai_data_*.yml` - Detailed processing data with prompts and outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d579d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Performance Comparison\n",
    "# Compare performance: single-threaded vs multi-threaded\n",
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST 9: Performance Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test with smaller dataset or heuristic mode for speed\n",
    "orchestrator = AIPipelineOrchestrator()\n",
    "orchestrator.load_data(databook_path, entity_name, 'BS')\n",
    "\n",
    "# Take only first 5 items for testing\n",
    "test_keys = workbook_list[:5]\n",
    "test_dfs = {k: dfs[k] for k in test_keys if k in dfs}\n",
    "\n",
    "print(f\"\\nTesting with {len(test_keys)} items\")\n",
    "\n",
    "# Test 1: Without multi-threading\n",
    "print(\"\\n--- Test 1: Single-threaded ---\")\n",
    "from fdd_utils.content_generation import ai_pipeline_full\n",
    "\n",
    "start = time.time()\n",
    "results_single = ai_pipeline_full(\n",
    "    mapping_keys=test_keys,\n",
    "    dfs=test_dfs,\n",
    "    model_type='local',\n",
    "    language='Eng',\n",
    "    use_heuristic=True,\n",
    "    use_multithreading=False\n",
    ")\n",
    "time_single = time.time() - start\n",
    "\n",
    "print(f\"Time: {time_single:.2f}s\")\n",
    "\n",
    "# Test 2: With multi-threading\n",
    "print(\"\\n--- Test 2: Multi-threaded (4 workers) ---\")\n",
    "start = time.time()\n",
    "results_multi = ai_pipeline_full(\n",
    "    mapping_keys=test_keys,\n",
    "    dfs=test_dfs,\n",
    "    model_type='local',\n",
    "    language='Eng',\n",
    "    use_heuristic=True,\n",
    "    use_multithreading=True,\n",
    "    max_workers=4\n",
    ")\n",
    "time_multi = time.time() - start\n",
    "\n",
    "print(f\"Time: {time_multi:.2f}s\")\n",
    "\n",
    "# Compare\n",
    "if time_multi > 0:\n",
    "    speedup = time_single / time_multi\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚ö° Speedup: {speedup:.2f}x faster with multi-threading\")\n",
    "    print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
